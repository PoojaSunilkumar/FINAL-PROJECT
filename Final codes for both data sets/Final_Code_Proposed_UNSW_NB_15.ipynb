{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vniw7A-Uur6k"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Several libraries are imported into this code to carry out necessary operations pertaining to data processing, modeling, scaling, warning suppression, and visualization. Pandas is used to manage and modify datasets; numpy is used to perform numerical computations; seaborn and matplotlib are used to generate visuals such as graphs and plots. The dataset is scaled using the MinMaxScaler from sklearn.preprocessing, and the training and testing sets are separated using train_test_split. GridSearchCV is then used to fine-tune the hyperparameters. To evaluate the performance of the model, a variety of evaluation measures are imported, including tools for creating confusion matrices and ROC curves, as well as accuracy, precision, recall, F1 score, and ROC-AUC. In order to avoid displaying needless caution messages, warnings are finally disabled."
      ],
      "metadata": {
        "id": "3CrzhztmTvF5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcxTyDFfdxJV"
      },
      "outputs": [],
      "source": [
        "#using the library to alter the data-set.\n",
        "import pandas as ppd\n",
        "#bringing in the infor-mation calculation methods\n",
        "import numpy as nym\n",
        "#Here, the libraries serve to draw the graph.\n",
        "import seaborn as sns\n",
        "#used this visualization library to create visual representations\n",
        "import matplotlib.pyplot as mpl\n",
        "# for scalling information-set we use the library\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# here we use the splitting module for dividing information\n",
        "from sklearn.model_selection import train_test_split,GridSearchCV\n",
        "# Importing variousn from the  module\n",
        "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,roc_auc_score,roc_curve,auc,confusion_matrix,classification_report\n",
        "# Suppress all caustions\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qj1J_mzuzBH"
      },
      "source": [
        "# Read Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmyI5dcogrGB",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# executing the information of examined data\n",
        "test_data =ppd.read_csv('/content/drive/MyDrive/UNSW_NB15_testing-set.csv')\n",
        "test_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ffPMvs1vCuY"
      },
      "source": [
        "# Read Train Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbQrdg1hjWbw",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "train_data=ppd.read_csv('/content/drive/MyDrive/UNSW_NB15_training-set.csv')\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYZSAz7AvIWz"
      },
      "source": [
        "# Dataset Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mIsPWPoj6kP",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "test_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvRIHzM9j-co",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "train_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QBnedKupxBZ"
      },
      "outputs": [],
      "source": [
        "com_df=ppd.concat([train_data,test_data],axis=0,ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_AoJJxxp37n",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "com_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvqz0FUAp8_n",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "com_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-W8MCumqCIR",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "com_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4GABdTgqNlw",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "com_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fR1QjmYIqTyD",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "com_df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuxOr9tFqZpO",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "com_df['attack_cat'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlgyavYevcDA"
      },
      "source": [
        "# Count Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKhLdOccromp",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Assuming the shaping is your merged DataFrame\n",
        "mpl.figure(figsize=(15,4))\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Creating the counting graph with a colorful palette\n",
        "ax = sns.countplot(x='attack_cat', data=com_df, palette='viridis')\n",
        "\n",
        "# Annotate the plotted with the counted of each category\n",
        "for p in ax.patches:\n",
        "    # Use the of the bar as the annotation text\n",
        "    ax.annotate(f'{p.get_height()}',(p.get_x() + p.get_width() / 2.,p.get_height()),\n",
        "                # Positioning the anno-tation at the center-top of the bar\n",
        "                ha='center',va='center',xytext=(0, 10),textcoords='offset points')\n",
        "\n",
        "# setting the header text in the plot\n",
        "mpl.title('Count Plot of Categories')\n",
        "# print the x-labl in the graph\n",
        "mpl.xlabel('Category')\n",
        "# print the y-labl in the graph\n",
        "mpl.ylabel('Count')\n",
        "# displaying the plot\n",
        "mpl.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkW8gx1xvfxB"
      },
      "source": [
        "# Drop Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjLJrKRmSjeh"
      },
      "outputs": [],
      "source": [
        "com_df.drop(columns=['id','sttl','dttl','ct_srv_src','ct_state_ttl','ct_dst_ltm','ct_dst_src_ltm','ct_src_ltm','ct_srv_dst'], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7p98gm8vi7F"
      },
      "source": [
        "# Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sj2ynF3btYIM"
      },
      "outputs": [],
      "source": [
        "# setting the library for encoding the data to numeric\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# fetching the columns that is objective\n",
        "object_cols=com_df.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Applying module to object columns\n",
        "label_encoder=LabelEncoder()\n",
        "# setting the lopping the method\n",
        "for col in object_cols:\n",
        "    # provide the data to the modules\n",
        "    com_df[col] = label_encoder.fit_transform(com_df[col])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUl9H0BOwIBo",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "com_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPTHdHfHvoIh"
      },
      "source": [
        "# Seprate Columns X and Y columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aROPIR8Gwq4W"
      },
      "outputs": [],
      "source": [
        "# select the columns and store in the variable\n",
        "X = com_df.drop('attack_cat',axis=1)\n",
        "# storing the target attributes in the y\n",
        "y = com_df['attack_cat']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WC9BYQnfvu3s"
      },
      "source": [
        "# Feture Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For classification problems, this code selects the top 20 features based on their ANOVA F-statistic (f_classif) using the SelectKBest feature selection algorithm from sklearn. To extract the most pertinent features, the approach is first applied to an existing dataset (X for features and y for labels). The chosen features' names and indices are retrieved, together with the scores that go along with them. For visualization, these scores as well as feature names are kept in a DataFrame and ordered descendingly by score. Lastly, a bar plot with appropriate titles and labels is created using Seaborn to represent the feature importance ratings. The plot is then displayed."
      ],
      "metadata": {
        "id": "5P1vgpQuj1gk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMLBR-tTKyFu",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# applying the method to get the essential qualities\n",
        "from sklearn.feature_selection import SelectKBest,f_classif\n",
        "\n",
        "k = 20\n",
        "# acquiring the values using methods\n",
        "selector=SelectKBest(score_func=f_classif,k=k)\n",
        "\n",
        "# Developing the methods to determine the characteristics of the object\n",
        "X_selected=selector.fit_transform(X,y)\n",
        "\n",
        "# Take out the appropriate attributes' names and indices.\n",
        "selected_features=selector.get_support(indices=True)\n",
        "# Getting the names and indices of the features.\n",
        "selected_feature_names=X.columns[selected_features]\n",
        "\n",
        "# Retrieve the characteristics-values\n",
        "feature_scores=selector.scores_[selected_features]\n",
        "\n",
        "# Make a data frame with the feature names in it.\n",
        "feature_scores_df=ppd.DataFrame({'Feature': selected_feature_names, 'Score': feature_scores})\n",
        "\n",
        "# Sort the Data-Frame by score for better visualization\n",
        "feature_scores_df=feature_scores_df.sort_values(by='Score',ascending=False)\n",
        "\n",
        "# give the size to the plot\n",
        "mpl.figure(figsize=(10, 6))\n",
        "# drawing the bar graph with selecting features\n",
        "sns.barplot(x='Score',y='Feature',data=feature_scores_df,palette='viridis')\n",
        "# setting the tittles of the graph\n",
        "mpl.title('Feature Importance Scores from SelectKBest')\n",
        "# setting the x-labl in the graph\n",
        "mpl.xlabel('Score')\n",
        "# give the y-labl in the graph\n",
        "mpl.ylabel('Feature')\n",
        "# displaying the graph\n",
        "mpl.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abzIe4yRVn-c",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "X[selected_feature_names].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd7ZNP_Mv2Mk"
      },
      "source": [
        "# Standard Scalling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sklearn.preprocessing module's usingr is used in this code to normalize the dataset. The first step in standardizing features is to construct a object, which scales to unit variance and removes the mean. After fitting the scaler to the data and converting it, the fit_transform method is performed to the subset of data. This produces X_scaled, a homogenized version of the original data with a mean of 0 and a standard deviation of 1."
      ],
      "metadata": {
        "id": "RlJBnqcyjF2p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1_VB4UZRIkg"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#normalizing the data by using the scalling module\n",
        "scaler=StandardScaler()\n",
        "\n",
        "# Transforming the data and fitting that scaler to it\n",
        "X_scaled=scaler.fit_transform(X[selected_feature_names])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XucpsmXev6Pv"
      },
      "source": [
        "# SMOTEENN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to balance an unbalanced dataset, this code illustrates how to apply the technique retrieved from the imblearn library. Initially, for reproducibility, a random seed is used to initialize the object. After that, the scaled characteristic matrix X_scaled as well as the target vector y are subjected to the fit_resample technique. By combining the two methods, a more balanced and clear dataset (X_resampled, y_resampled) is produced. SMOTE (Synthetic Minority Over-sampling Technique) creates synthetic samples for the minority class, while ENN (Edited Nearest Neighbors) eliminates noisy or unclear locations.\n"
      ],
      "metadata": {
        "id": "Rju6hgBRzeAG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejOL_fuoVxIa"
      },
      "outputs": [],
      "source": [
        "from imblearn.combine import SMOTEENN\n",
        "\n",
        "# showing this library to balanced the data\n",
        "smote_enn=SMOTEENN(random_state=42)\n",
        "\n",
        "# Fittng and resample the data-set\n",
        "X_resampled,y_resampled=smote_enn.fit_resample(X_scaled, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWltu2QNv_Cd"
      },
      "source": [
        "# Balance Count Plot"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code first creates a DataFrame from the recreated data (X_resampled and y_resampled), naming the features dynamically as Feature_0, Feature detector_1, and so forth. Y_resampled, which stands for the target labels, is added as a new column called 'attack_cat'. The class distribution found in the 'attack_cat' column is then visualized using a bar plot made with Seaborn's countplot, which indicates how many instances, following the SMOTEENN resampling procedure, belong to each class. mpl.show() is used to display the customized plot, which has a title, named axes, and a color scheme (viridis)."
      ],
      "metadata": {
        "id": "jg-rvqmUEDe-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_h5dZssZ6ql",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Converting resampling data to Data-Frame\n",
        "resampled_df=ppd.DataFrame(X_resampled,columns=[f'Feature_{i}' for i in range(X_resampled.shape[1])])\n",
        "resampled_df['attack_cat']=y_resampled\n",
        "# Plotting the class distri-bution\n",
        "mpl.figure(figsize=(10, 6))\n",
        "# drawing the counting graph with the specific data\n",
        "sns.countplot(x='attack_cat',data=resampled_df, palette='viridis')\n",
        "# setting the tittles of the graph\n",
        "mpl.title('Class Distribution After SMOTEENN')\n",
        "# printing the x-labls in the plot\n",
        "mpl.xlabel('Class')\n",
        "# adding the labls in the plot\n",
        "mpl.ylabel('Count')\n",
        "# visualizing the dataset\n",
        "mpl.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsxYqX5BwENh"
      },
      "source": [
        "# Splitting Training & Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELB2Zf_57r0z"
      },
      "outputs": [],
      "source": [
        "#Creating data sets for both training and assessment using the updated information.\n",
        "X_train,X_test,y_train,y_test=train_test_split(X_resampled,y_resampled,test_size=0.2,random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d9pEIg9wKmR"
      },
      "source": [
        "# Define Function"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines the function evaluate_result(), which computes and visualizes a number of metrics to assess a classification model's performance. Using weighted averages for multi-class classification, it first outputs the model's precision, recall, accuracy, and F1 score based on the real labels (y_true) and predicted labels (y_pred). It has the option to calculate and report the ROC-AUC score in the event that projected probabilities (y_pred_proba) are given. The function then outputs a thorough classification report and shows the link between the true and predicted classes in a heatmap visualization of the confusion matrix. This aids in evaluating the error distribution and overall performance of the model."
      ],
      "metadata": {
        "id": "6aeFMF3CZo55"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxg9oQi47HxY"
      },
      "outputs": [],
      "source": [
        "def evaluate_result(y_true,y_pred,y_pred_proba=None):\n",
        "    # evalute the accu from the data\n",
        "    print(\"Accuracy: {:.8f}\".format(accuracy_score(y_true,y_pred)))\n",
        "\n",
        "     # use the testing data to compute the preci-vlaue.\n",
        "    print(\"Precision: {:.8f}\".format(precision_score(y_true,y_pred,average='weighted')))\n",
        "\n",
        "    # get the reca-value withe the data\n",
        "    print(\"Recall: {:.8f}\".format(recall_score(y_true,y_pred,average='weighted')))\n",
        "\n",
        "    # discover the f1socre's value\n",
        "    print(\"F1_Score: {:.8f}\".format(f1_score(y_true,y_pred,average='weighted')))\n",
        "\n",
        "    # Drawing the classied tabel with the result\n",
        "    report=classification_report(y_true,y_pred)\n",
        "    # showing the result table\n",
        "    print(\"Classification Report:\\n\",report)\n",
        "\n",
        "    # draw the matrics of plot confusing\n",
        "    cm=confusion_matrix(y_true,y_pred)\n",
        "    # draw the confusing matrics of the data\n",
        "    mpl.figure(figsize=(10, 6))\n",
        "    # printing the xlabel to the model\n",
        "    sns.heatmap(cm,annot=True,fmt=\"d\",cmap=\"Blues\",cbar=False)\n",
        "    # use the y label in the graph\n",
        "    mpl.xlabel(\"Predicted\")\n",
        "    # show the header in the graph\n",
        "    mpl.ylabel(\"True\")\n",
        "    # adding tittles in the graph\n",
        "    mpl.title(\"Confusion Matrix\")\n",
        "    # display the matrix\n",
        "    mpl.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To depict the Receiver Operating Characteristic (ROC) curves for a classification with multiple classes problem, this code defines the plot_multiclass_roc function. In order to compute the ROC curve, the area under the curve scores, and binary labels, it first imports the required libraries from sklearn. Next, the function receives as inputs the predicted probabilities (y_pred_proba) and true labels (y_true). It does this by converting the true labels into a binary format, calculating the true positive rates (TPR) and false positive rates (FPR) for every class, and then outputting the AUC (Area Under the Curve) for every class. Additionally, it computes the AUC for the micro-average of the ROC curve, essentially aggregates all classes.The function then creates a plot of the ROC curve for each class in various colors, adds a micro-average curve, as well as presents the plot with the relevant labels, titles, and legends."
      ],
      "metadata": {
        "id": "BQnO3tYjZTSt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMlDfDeg-jkD"
      },
      "outputs": [],
      "source": [
        "# importing the libray with the result\n",
        "from sklearn.metrics import roc_curve,roc_auc_score,auc\n",
        "# setting this tag as label in the plot\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "def plot_multiclass_roc(y_true,y_pred_proba):\n",
        "    # show the chamber of the data\n",
        "    n_classes=y_pred_proba.shape[1]\n",
        "\n",
        "    # Make the output labels binary.\n",
        "    y_true_bin=label_binarize(y_true,classes=nym.arange(n_classes))\n",
        "\n",
        "    #Calculating each class's ROC curve as well as ROC AUC\n",
        "    fpr={}\n",
        "    tpr={}\n",
        "    roc_auc={}\n",
        "    # Looping through to compute the curve.\n",
        "    for i in range(n_classes):\n",
        "        #Calculating the socre readings as the probability of the estimator\n",
        "        fpr[i], tpr[i], _ =roc_curve(y_true_bin[:, i],y_pred_proba[:, i])\n",
        "        # Identifying the values in the graph's categories\n",
        "        roc_auc[i]=auc(fpr[i],tpr[i])\n",
        "         # With eight decimal places of precision, print every ROC-AUC score associated with the i-th class..\n",
        "        print(f\"Class {i} ROC-AUC: {roc_auc[i]:.8f}\")\n",
        "    #Flattening the true basic labels to derive the area under the  curve.\n",
        "    fpr[\"micro\"], tpr[\"micro\"], _ =roc_curve(y_true_bin.ravel(),y_pred_proba.ravel())\n",
        "    # Calculating the AUC for the curve.\n",
        "    roc_auc[\"micro\"]=auc(fpr[\"micro\"],tpr[\"micro\"])\n",
        "    # Printing the ROC-verage score with eight decimal places.\n",
        "    print(f\"Micro-average ROC-AUC: {roc_auc['micro']:.8f}\")\n",
        "\n",
        "    # Plotting all ROC curves\n",
        "    mpl.figure(figsize=(10, 8))\n",
        "    # Generating a list of colors using the rainbow colormap\n",
        "    colors = mpl.cm.rainbow(nym.linspace(0, 1, n_classes))\n",
        "    # Looping through each class and its corresponding color\n",
        "    for i, color in zip(range(n_classes),colors):\n",
        "        # Plotting the ROC curve for each class with its corresponding color\n",
        "        mpl.plot(fpr[i],tpr[i],color=color,lw=2,label=f'ROC curve for class {i} (AUC = {roc_auc[i]:.2f})')\n",
        "    # Plotting curve, which is a performance measure that aggregates across all classes\n",
        "    mpl.plot(fpr[\"micro\"],tpr[\"micro\"],color='navy',lw=2, linestyle=':',label=f'Micro-average ROC curve (AUC = {roc_auc[\"micro\"]:.2f})')\n",
        "    # Drawing a line along the  as (1, 1) to show of a random classifier.\n",
        "    mpl.plot([0, 1],[0, 1],color='gray',linestyle='--')\n",
        "    #Adding labels to the x-lbls in the plots\n",
        "    mpl.xlim([0.0, 1.0])\n",
        "    # Setting the y-axis limits to range from 0.0 to 1.05 to allow for some space\n",
        "    mpl.ylim([0.0, 1.05])\n",
        "    # Setting the x-labls in the graph\n",
        "    mpl.xlabel('False Positive Rate')\n",
        "    # adding the y-axis labls as the plot\n",
        "    mpl.ylabel('True Positive Rate')\n",
        "    #printing the titrtles of the plot\n",
        "    mpl.title('Receiver Operating Characteristic (ROC) Curve for Multiclass Classification')\n",
        "    # Adding a legending to the plot\n",
        "    mpl.legend(loc=\"lower right\")\n",
        "    # visualizing the graph\n",
        "    mpl.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra Tree Classifier"
      ],
      "metadata": {
        "id": "hYM1fCItxB-H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates an ExtraTreesClassifier model and uses RandomizedSearchCV to hyperparameter tune it in order to determine the optimal set of parameters. The maximal depth (maximum tree depth), min_samples_leaf (minimum samples needed for a leaf node), and split_samples (minimum samples needed to split a node) are specified in a parameter grid that is defined. Five iterations (n_iter=5) are performed by RandomizedSearchCV in order to examine various parameter combinations. Using the adjusted parameters, the model gets trained on X_train, and the top model (bst_mdel) is chosen based on the search outcomes. Next, using the best model, predictions are generated on the training (X_train) as well as testing (X_test) data."
      ],
      "metadata": {
        "id": "_iLmysn5DxJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using the algorithms with specific para-meter tuning\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "# apply the para-meter tuning to the model\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Defining model with the creating alias\n",
        "model_etc = ExtraTreesClassifier()\n",
        "\n",
        "# Expanding the parameter grid for the alogrithms\n",
        "param_grid = {\n",
        "    # List of tree rates to test\n",
        "    'n_estimators': [50,60,],\n",
        "    # tabel of learning rates to test\n",
        "    'max_depth': [10, 20],\n",
        "    # Listing of maximum depths to test\n",
        "    'min_samples_split': [2, 5],\n",
        "    # use leaf value with number\n",
        "    'min_samples_leaf': [1, 2]\n",
        "\n",
        "}\n",
        "\n",
        "#setting the value of this approach using the argument\n",
        "radm_sech=RandomizedSearchCV(estimator=model_etc,param_distributions=param_grid,n_iter=5,verbose=2,random_state=42,n_jobs=-1)\n",
        "\n",
        "# Match the training data to the model.\n",
        "radm_sech.fit(X_train,y_train)\n",
        "\n",
        "# Using para- metert tuning to choose the best model.\n",
        "bst_mdel=radm_sech.best_estimator_\n",
        "\n",
        "# Predicting on the trained data\n",
        "y_trn_prd_etc=bst_mdel.predict(X_train)\n",
        "# calculating the prediction with the tested data\n",
        "y_tst_prd_etc=bst_mdel.predict(X_test)"
      ],
      "metadata": {
        "id": "1rmiPBAhltYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the probability values for the training and testing data\n",
        "y_trn_prd_proba_etc=bst_mdel.predict_proba(X_train)\n",
        "# evaluting value of probality for roc curve\n",
        "y_tst_prd_proba_etc=bst_mdel.predict_proba(X_test)"
      ],
      "metadata": {
        "id": "E7c-Dm1XrBSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# showing the vlaue of good param-meter\n",
        "print(\"Best Parameters:\", radm_sech.best_params_)"
      ],
      "metadata": {
        "id": "pwOYySx_qK9D",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Result"
      ],
      "metadata": {
        "id": "ihY1iLX-wlKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_result(y_test,y_tst_prd_etc)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Rrx3EhLJrLwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_multiclass_roc(y_test,y_tst_prd_proba_etc)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "elQ7O8fPwe2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Result"
      ],
      "metadata": {
        "id": "J5mGXmImwrjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_result(y_train,y_trn_prd_etc)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DKiggIf_wuBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_multiclass_roc(y_train,y_trn_prd_proba_etc)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zYCE-B0Hwy0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h12342o6wW5S"
      },
      "source": [
        "# XGboost Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates a parameter grid (param_dist) allowing hyperparameter tweaking using RandomizedSearchCV and initializes new XGBoost classifier (xgb.XGBClassifier()). The learning rate (learning_rate), maximum depth (max_depth), and number inside estimators (n_estimators) of the trees are among the parameters being adjusted. With n_iter=5, the RandomizedSearchCV is set up to run five iterations while maximizing accuracy. The best estimator (bst_xgb_clf) is chosen after the model has been trained using the training dataset (X_train, y_train). The training and verification datasets (y_train_pred, y_test_pred) are forecasted using this best model, and projected probabilities (y_train_proba, y_test_proba) additionally obtained."
      ],
      "metadata": {
        "id": "qkHD7s46q09i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdTcxgHj63CJ"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Initializing the algorithms with the para-meter technique\n",
        "xgb_clf=xgb.XGBClassifier()\n",
        "\n",
        "# Defining the para-meter grid for tuning method\n",
        "param_dist = {\n",
        "    # List of tree rates to test\n",
        "    'n_estimators': [10,20],\n",
        "     #learning-rates table for assessment\n",
        "    'learning_rate': [0.01, 0.2],\n",
        "    # Listing of maximum depths to test\n",
        "     'max_depth': [3, 4]\n",
        "}\n",
        "\n",
        "#setting the value of this approach using the argument\n",
        "rom_srch=RandomizedSearchCV(xgb_clf,param_distributions=param_dist,n_iter=5,scoring='accuracy',verbose=1)\n",
        "\n",
        "# Align the model with its initial set of data\n",
        "rom_srch.fit(X_train,y_train)\n",
        "\n",
        "# Acquire the ideal settings and scoring\n",
        "bst_xgb_clf=rom_srch.best_estimator_\n",
        "# Selecting the best model to predict considering inforamtion\n",
        "y_train_pred=bst_xgb_clf.predict(X_train)\n",
        "# Making predictions with test information and creating the best possible model\n",
        "y_test_pred=bst_xgb_clf.predict(X_test)\n",
        "\n",
        "# Getting predicted probabilities with data\n",
        "y_test_proba=bst_xgb_clf.predict_proba(X_test)\n",
        "# Using data to get believed the likelihood\n",
        "y_train_proba=bst_xgb_clf.predict_proba(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfuRNaUhAFud"
      },
      "outputs": [],
      "source": [
        "y_train_proba=best_xgb_clf.predict_proba(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNSXUAjL-M5q"
      },
      "source": [
        "# Testing Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3sV-FBm_UoO",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# showing the vlaue of good param-meter\n",
        "print(\"Best Parameters:\", rom_srch.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jTMdVr8Hmv0",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "evaluate_result(y_test,y_test_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vZwWsbvQ-5XJ"
      },
      "outputs": [],
      "source": [
        "plot_multiclass_roc(y_test,y_test_proba)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPC4HU20_s0x"
      },
      "source": [
        "# Training Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZUPdtV5ItoZ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "evaluate_result(y_train,y_train_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CU2pvm0r_7Bv",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "plot_multiclass_roc(y_train,y_train_proba)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sOP6_3TwiPj"
      },
      "source": [
        "# Reshaping Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code restructures both the testing and the training feature so that each sample's data is represented as a single feature per timestep, creating a 3D structure that is appropriate for input into an LSTM layer. To ensure compliance with LSTM networks, which require input in the form of (samples, timesteps, features), the reshape approach adjusts the final dimension. To further help the model handle classification issues with multiple classes more successfully, the to_categorical function provided Keras is used to transform the target labels from a one-hot encoded format."
      ],
      "metadata": {
        "id": "MM6R31lw2bUa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dlxiTNNABmhF"
      },
      "outputs": [],
      "source": [
        "# reshaping the final dimension to include one feature per timestep\n",
        "x_train_reshaped=X_train.reshape(X_train.shape[0],X_train.shape[1], 1)\n",
        "# Reshaping the last dimension to have 1 feature per timestep, which is required by the LSTM layer\n",
        "x_test_reshaped=X_test.reshape(X_test.shape[0],X_test.shape[1], 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4MHh3RECbry"
      },
      "outputs": [],
      "source": [
        "# use this library for convert the information\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# getting the value of the score of the data.\n",
        "y_train_categorical=to_categorical(y_train)\n",
        "# fetching data from the library\n",
        "y_test_categorical=to_categorical(y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NrjEMSmrCn_s"
      },
      "outputs": [],
      "source": [
        "! pip install -U keras-tuner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgWniP1FwnwI"
      },
      "source": [
        "# LSTM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Keras and Keras Tuner, this code creates and optimizes an LSTM-based deep learning algorithm to perform multi-class categorization. It imports the required optimizers (Adam, SGD, RMSprop, and Adadelta) and libraries (LSTM, Dropout, along with Dense layers from TensorFlow). A sequential model with adjustable hyperparameters, including LSTM units, dropout rates, as well as optimizer types, is constructed via the build_mdl function. Two LSTM layers (with configurable units), dropout layers to avoid overfitting, including dense layers for classification—including an output layer supporting multi-class classification that is triggered by softmax—make up the model. An accuracy measure and a categorical crossover entropy loss function are used to create the model. The best hyperparameters are found by experimenting with the training set of data using Keras Tuner's RandomSearch. The ideal model is constructed and trained on the shaped training data once the best hyperparameters have been determined. The performance is then assessed using the validation data.The optimal hyperparameters are provided at the end."
      ],
      "metadata": {
        "id": "aFKcAjk-a-8z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "j1eLonx1DzR1"
      },
      "outputs": [],
      "source": [
        "# Importing a library for hyperparameter optimization\n",
        "import kerastuner as kt\n",
        "#In order to develop neural networks, modeling class\n",
        "from tensorflow.keras.models import Sequential\n",
        "# here we use these library for developing the deep learning algorithms\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "# evaluating the performance on classi-fication using the values.\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta\n",
        "\n",
        "def build_mdl(hp):\n",
        "    mdl = Sequential()\n",
        "\n",
        "    #Tunable units with respective return sequences are added to the layer\n",
        "    mdl.add(LSTM(units=hp.Int('lstm_units',min_value=32,max_value=128,step=32),return_sequences=True,\n",
        "                   # giving the shaping of information as inputing to the estimaor\n",
        "                   input_shape=(x_train_reshaped.shape[1],x_train_reshaped.shape[2])))\n",
        "    # Adding lyr with tunable dropout rate to prevent overfitting\n",
        "    mdl.add(Dropout(rate=hp.Float('dropout',min_value=0.2,max_value=0.5,step=0.1)))\n",
        "    # modfied-layr, also with tunable unitting but no returned sequences\n",
        "    mdl.add(LSTM(units=hp.Int('lstm_units',min_value=32,max_value=128,step=32),return_sequences=False))\n",
        "    # To lesen overfiting, add another-layer with an adjustable-rate.\n",
        "    mdl.add(Dropout(rate=hp.Float('dropout',min_value=0.2,max_value=0.5,step=0.1)))\n",
        "    # Fully conected lyr with tunabling unitted and methosds\n",
        "    mdl.add(Dense(units=hp.Int('dense_units',min_value=32,max_value=128,step=32), activation='relu'))\n",
        "    #For multi_clas clasification, the outpt layr and softmax activated\n",
        "    mdl.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    # Selecting optimizer based on tunable choice\n",
        "    optimizer_name=hp.Choice('optimizer',values=['adam','sgd','rmsprop','adadelta'])\n",
        "    # Checking if the selected optimizering is vlues\n",
        "    if optimizer_name == 'adam':\n",
        "        # provding vluesw the scoreing\n",
        "        optimizer = Adam()\n",
        "\n",
        "    elif optimizer_name == 'sgd':\n",
        "        optimizer = SGD()\n",
        "    #create an instance of the Stochastic Gradient Descent\n",
        "    elif optimizer_name == 'rmsprop':\n",
        "        optimizer=RMSprop()\n",
        "    else:\n",
        "        optimizer = Adadelta()\n",
        "    # filtterd algortimsn with the specific para-meter vlaues in the mdls\n",
        "    mdl.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "    return mdl\n",
        "\n",
        "# setting data of the reshpaed in the algorthms\n",
        "tuner=kt.RandomSearch(build_mdl,objective='val_accuracy',max_trials=5,executions_per_trial=1)\n",
        "\n",
        "# Looking for the optimal hyper-para-meter settings\n",
        "tuner.search(x_train_reshaped,y_train_categorical,epochs=5,validation_data=(x_test_reshaped,y_test_categorical))\n",
        "\n",
        "# Choosing the opti-mal hyper-pra-metrs and framework.\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"Best hyperparameters: LSTM Units: {best_hps.get('lstm_units')}, Dense Units: {best_hps.get('dense_units')}, \"\n",
        "      f\"Dropout: {best_hps.get('dropout')}, Optimizer: {best_hps.get('optimizer')}\")\n",
        "\n",
        "# Constructing the ideal modeling\n",
        "best_mdl=tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Trained the top mannequin\n",
        "history=best_mdl.fit(x_train_reshaped,y_train_categorical,epochs=5,validation_data=(x_test_reshaped,y_test_categorical))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculated-prediction of the value\n",
        "y_train_pred=best_mdl.predict(x_train_reshaped)\n",
        "# findout teh vlaue of predicition of the mdl\n",
        "y_test_pred=best_mdl.predict(x_test_reshaped)\n",
        "# findout the vlaue of testing data\n",
        "y_train_classes=nym.argmax(y_train_categorical,axis=1)\n",
        "# evaluate- result of the mdl\n",
        "y_test_classes=nym.argmax(y_test_categorical,axis=1)\n",
        "## showfoe result of the mdl with the data\n",
        "trn_prd_clases=nym.argmax(y_train_pred,axis=1)\n",
        "#Determine the value of the test data.\n",
        "tet_ped_clases=nym.argmax(y_test_pred,axis=1)"
      ],
      "metadata": {
        "id": "uFZRaf1sy3PQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XljngJoXKNr_",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Assign probability to test and prepare the data.\n",
        "y_train_roc=best_mdl.predict(x_train_reshaped)\n",
        "# Calculate the probability for both test and training information sets.\n",
        "y_test_roc=best_mdl.predict(x_test_reshaped)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NPRPYUYJhTs"
      },
      "source": [
        "# Testing Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuUQUA4pQxni",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "evaluate_result(y_test_classes, tet_ped_clases)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LGSfQJU1Kx2b"
      },
      "outputs": [],
      "source": [
        "plot_multiclass_roc(y_test_classes,y_test_roc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRzgPYiwM4EV"
      },
      "source": [
        "# Training Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BNnWgqvcM64d"
      },
      "outputs": [],
      "source": [
        "evaluate_result(y_train_classes, trn_prd_clases)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ckaxdsqyM93P"
      },
      "outputs": [],
      "source": [
        "plot_multiclass_roc(y_train_classes,y_train_roc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7V0wCd_L4Ci"
      },
      "source": [
        "# **Before** **Balancing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEn_HU6UL8vH"
      },
      "outputs": [],
      "source": [
        "# Dividing the refreshed information into data sets for testing as well as training.\n",
        "X_train_un,X_test_un,y_train_un,y_test_un=train_test_split(X_scaled,y,test_size=0.2,random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra Tree Classifier"
      ],
      "metadata": {
        "id": "hKc-MQWLzJxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining th model with the specific para-meter\n",
        "model_etc=ExtraTreesClassifier()\n",
        "\n",
        "# Expand the parameter grid for with the algorithms\n",
        "param_grid = {\n",
        "    # Increase number of trees\n",
        "    'n_estimators': [50,60,],\n",
        "    # Allow deeper trees\n",
        "    'max_depth': [10, 20],\n",
        "    # Increase range for splitting\n",
        "    'min_samples_split': [2, 5],\n",
        "    # Different strategies for feature selection\n",
        "    'min_samples_leaf': [1, 2]\n",
        "\n",
        "}\n",
        "\n",
        "# Adjusting the hyperpamters using this specific frame-work.\n",
        "ram_srch=RandomizedSearchCV(estimator=model_etc,param_distributions=param_grid,n_iter=5,verbose=2,random_state=42,n_jobs=-1)\n",
        "\n",
        "# Fit the model to the training data\n",
        "ram_srch.fit(X_train_un,y_train_un)\n",
        "\n",
        "# Getting model from RandomizedSearchCV\n",
        "bst_modl=ram_srch.best_estimator_\n",
        "\n",
        "# Predict on the training and testing data\n",
        "trn_prd_etc=bst_modl.predict(X_train_un)\n",
        "tst_prd_etc=bst_modl.predict(X_test_un)\n",
        "# Calculate the probability values for the training and testing data\n",
        "trn_proba_etc=bst_modl.predict_proba(X_train_un)\n",
        "tst_proba_etc=bst_modl.predict_proba(X_test_un)"
      ],
      "metadata": {
        "id": "CqhuHFktxfYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# showing the vlaue of good parameter\n",
        "print(\"Best Parameters:\", ram_srch.best_params_)"
      ],
      "metadata": {
        "id": "a5_SEsUUyW1L",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Result"
      ],
      "metadata": {
        "id": "VQMaFcOKy2w8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_result(y_test_un,tst_prd_etc)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pEvbLM1nyabK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_multiclass_roc(y_test_un,tst_proba_etc)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "bqP77qZqyxv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Result"
      ],
      "metadata": {
        "id": "fj4c9jRay6Ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_result(y_train_un,trn_prd_etc)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-fCdygatynlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_multiclass_roc(y_train_un,trn_proba_etc)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FmrNV41ry-VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiTsDiNRxCVK"
      },
      "source": [
        "# XGBoost Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZDj0diPMVDP"
      },
      "outputs": [],
      "source": [
        "# Initializing the algorithms with the specific values\n",
        "xgb_clf=xgb.XGBClassifier()\n",
        "\n",
        "# Defining the parameter grid for valeus\n",
        "param_dist = {\n",
        "    # Listing of tree rates to test\n",
        "    'n_estimators': [10,20],\n",
        "     # use the vlaue of rate of the model\n",
        "    'learning_rate': [0.01, 0.2],\n",
        "\n",
        "}\n",
        "\n",
        "# How the value is applied to the parametric procedure\n",
        "radon_sch=RandomizedSearchCV(xgb_clf,param_distributions=param_dist,n_iter=5, scoring='accuracy', verbose=1)\n",
        "\n",
        "# Fit the RandomizedSearchCV\n",
        "radon_sch.fit(X_train_un, y_train_un)\n",
        "\n",
        "# Get the best model\n",
        "bet_xgb_clf=radon_sch.best_estimator_\n",
        "# Make predictions\n",
        "y_tran_pd_xg=bet_xgb_clf.predict(X_train_un)\n",
        "y_tst_pd_xg=bet_xgb_clf.predict(X_test_un)\n",
        "# Get predicted probabilities\n",
        "tst_proba=bet_xgb_clf.predict_proba(X_test_un)\n",
        "tran_proba=bet_xgb_clf.predict_proba(X_train_un)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kS-cGCR1N9Rf",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# showing the vlaue of good param-meter\n",
        "print(\"Best Parameters:\", radon_sch.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQLVtDmGONH8"
      },
      "source": [
        "# Testing Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbfHtEUDHH2j",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "evaluate_result(y_test_un,y_tst_pd_xg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "590jDxLUOfhU"
      },
      "outputs": [],
      "source": [
        "plot_multiclass_roc(y_test_un,tst_proba)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN1FVMtYOpfN"
      },
      "source": [
        "# Training Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jtL8s5wrOsM_"
      },
      "outputs": [],
      "source": [
        "evaluate_result(y_train_un,y_tran_pd_xg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9_SbXBtjOx15"
      },
      "outputs": [],
      "source": [
        "plot_multiclass_roc(y_train_un,tran_proba)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4aXI1hOxKIi"
      },
      "source": [
        "# Reshaping Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j--Dvd62n7Og",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Reshape X_train and X_test for use in models that require 3D input\n",
        "x_train_reshaped_un=X_train_un.reshape(X_train_un.shape[0], X_train_un.shape[1], 1)\n",
        "x_test_reshaped_un=X_test_un.reshape(X_test_un.shape[0], X_test_un.shape[1], 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qtThx-FovDD"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Assuming y_train and y_test are your labels.\n",
        "y_train_categorical_un=to_categorical(y_train_un)\n",
        "y_test_categorical_un=to_categorical(y_test_un)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "my5i0NXHGQsF",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "y_test_categorical_un.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZMi-RiZxQLB"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Keras Tuner to optimize hyperparameters, this code creates and constructs an LSTM-based artificial intelligence model. By using hp.Int and hp.Float to dynamically set the amount of LSTM components, dropout rate, which serves and dense layer units, the function build_mol(hp) generates a sequential model that can be modified across a given range. The model consists of two layers: an LSTM layer who returns sequences and uses dropout for regularization, and another LSTM layer that also uses dropout. Softmax activation is used in the final output layer for multi-class classification, and a dense layer is added with ReLU activation.Based on a choice of hyperparameters, the optimizer is chosen from among \"adam,\" \"sgd,\" \"rmsprop,\" or \"adadelta.\"Categorical cross-entropy loss and accuracy are used as the metrics in the model's compilation.Then, the kt.RandomSearchtuner is used to find the optimal set of hyperparameters, including the number of LSTM units, dense units, dropout rates, appropriate optimizer. Following tuning, three epochs of training data are used to train the model, and validation data is supplied for assessment throughout training. The optimal hyperparameters are then reported."
      ],
      "metadata": {
        "id": "QMaoAkuccRTf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IF9qk9qUo6Mw",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Explain how lstm The model works\n",
        "def build_mol(hp):\n",
        "   # Construct a ml in sequence\n",
        "    mol = Sequential()\n",
        "\n",
        "    # Include a layer depending on the given settings\n",
        "    mol.add(LSTM(units=hp.Int('lstm_units',min_value=32,max_value=128,step=32),return_sequences=True,\n",
        "\n",
        "                   # Define the convolutional network layer\n",
        "                   input_shape=(x_train_reshaped_un.shape[1],x_train_reshaped_un.shape[2])))\n",
        "    # Adding a layer with a rate selected from a range\n",
        "    mol.add(Dropout(rate=hp.Float('dropout',min_value=0.2,max_value=0.5,step=0.1)))\n",
        "    # Fitting a metoding for dimensionality reduction\n",
        "    mol.add(LSTM(units=hp.Int('lstm_units',min_value=32,max_value=128,step=32),return_sequences=False))\n",
        "    # Add a densing layerings with specified activated function\n",
        "    mol.add(Dropout(rate=hp.Float('dropout',min_value=0.2,max_value=0.5,step=0.1)))\n",
        "    # A choose from layer can be added to add legalization\n",
        "    mol.add(Dense(units=hp.Int('dense_units',min_value=32,max_value=128, step=32),activation='relu'))\n",
        "   # Including a lyr with two parameters and units\n",
        "    mol.add(Dense(10,activation='softmax'))\n",
        "\n",
        "    # Optimazoring\n",
        "    optimizer_name=hp.Choice('optimizer',values=['adam','sgd','rmsprop','adadelta'])\n",
        "    # Checking if the selected optimizered\n",
        "    if optimizer_name== 'adam':\n",
        "        # If true, set the optimizing to Adam optim\n",
        "        optimizer=Adam()\n",
        "    # Checking if the selected optimizer\n",
        "    elif optimizer_name=='sgd':\n",
        "        # If true, set the optimizering to SGD\n",
        "        optimizer=SGD()\n",
        "    elif optimizer_name=='rmsprop':\n",
        "        optimizer=RMSprop()\n",
        "    else:\n",
        "        optimizer=Adadelta()\n",
        "    #Constructing the mdl using the given metrics.\n",
        "    mol.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "    return mol\n",
        "\n",
        "# using Valeus to apply the para-meter tuning in the estimator\n",
        "tuner=kt.RandomSearch(build_mol,objective='val_accuracy',max_trials=5,executions_per_trial=1)\n",
        "\n",
        "#Trying to find the ideal hyper-parameters\n",
        "tuner.search(x_train_reshaped_un,y_train_categorical_un,epochs=5,validation_data=(x_test_reshaped_un,y_test_categorical_un))\n",
        "\n",
        "#Getting the best model performance with hyper-para-meters\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "# displaying the goot para-valeu of the algorithms with each points\n",
        "print(f\"Best hyperparameters: LSTM Units: {best_hps.get('lstm_units')}, Dense Units: {best_hps.get('dense_units')}, \"\n",
        "      f\"Dropout: {best_hps.get('dropout')}, Optimizer: {best_hps.get('optimizer')}\")\n",
        "\n",
        "# creating the best algorithms with specific values\n",
        "best_mol=tuner.hypermodel.build(best_hps)\n",
        "\n",
        "#providing the trained data to the algorimthsm\n",
        "history=best_mol.fit(x_train_reshaped_un,y_train_categorical_un,epochs=3,validation_data=(x_test_reshaped_un,y_test_categorical_un))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considering a trained model (best_mol), this code predicts and converts labels for training and testing data. It begins by predicting the labels for the reshaped training data (x_train_reshaped_un), then uses argmax to take the index of the highest probability and translate the predicted probabilities into class labels. In a same manner, it turns the training data's real category categories into class labels. The procedure is repeated with the test data: both the true and predicted category labels are transformed into class labels, and the model predictions the labels for the rearranged test data (x_test_reshaped_un). This makes it possible to compare the genuine and anticipated labels for assessment reasons."
      ],
      "metadata": {
        "id": "iym579CHhY6-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_mppo1HmoRv",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Identifying labels for the simulated data using the most effective model\n",
        "y_train_pred=best_mol.predict(x_train_reshaped_un)\n",
        "# Use the index to translate the anticipated probability into class labels.\n",
        "ytrn_pd_clases=nym.argmax(y_train_pred, axis=1)\n",
        "#Translating the authentic labels into class labels\n",
        "ytin_cls=nym.argmax(y_train_categorical_un, axis=1)\n",
        "\n",
        "# Predicted the categories for the experimental data using the most effective model.\n",
        "y_tst_ped_lst=best_mol.predict(x_test_reshaped_un)\n",
        "#Taking the index and converting the anticipated probability to class labels\n",
        "y_test_pred_classes=nym.argmax(y_tst_ped_lst, axis=1)\n",
        "# Making the class labels from the genuine labels\n",
        "y_test_classes=nym.argmax(y_test_categorical_un, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gcw7WEK3ngsu",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# getting the vlues for the the curved\n",
        "y_train_prob=best_mol.predict(x_train_reshaped_un)\n",
        "# obtaining the curved values for the\n",
        "y_test_prob=best_mol.predict(x_test_reshaped_un)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYZb1xiPkDlP"
      },
      "source": [
        "# Testing Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXwkoqp_NscY",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "evaluate_result(y_test_classes,y_test_pred_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1gvxhSKRn0Gf"
      },
      "outputs": [],
      "source": [
        "plot_multiclass_roc(y_test_classes,y_test_prob)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0RQjmBukGRC"
      },
      "source": [
        "# Training Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtJVS2jPN8U1",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "evaluate_result(ytin_cls,ytrn_pd_clases)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zgEvUm4FoCFw"
      },
      "outputs": [],
      "source": [
        "plot_multiclass_roc(ytin_cls,y_train_prob)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
